# -*- coding: utf-8 -*-
"""LLM-Fine-Tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b_510RFqP1vYoasJgVR6WWWNWMLQ5A7e
"""

from google.colab import drive
drive.mount('/content/drive')

# Set persistent path for saving model
output_dir = '/content/drive/MyDrive/llama2_reft/trained_intervention'
workspace_dir = '/content/drive/MyDrive/llama2_reft/workspace'

import os

os.makedirs(save_dir, exist_ok=True)
os.makedirs(workspace_dir, exist_ok=True)

!unzip TrainingLLM.zip

!pip install pyreft

!pip install nnsight
!pip install colorama

import pandas as pd
df = pd.read_csv('TrainingLLM.csv')
X = df['Prompt'].values
y = df[' Response'].values

X

y

import torch, transformers, pyreft
import pandas as pd
from colorama import init, Fore
from huggingface_hub import login

init()

model_name = 'meta-llama/Llama-2-7b-chat-hf'
device = 'cuda' if torch.cuda.is_available() else 'cpu'

login(token='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')


model = transformers.AutoModelForCausalLM.from_pretrained(
    model_name, torch_dtype=torch.bfloat16, device_map=device,
    cache_dir=workspace_dir
)


tokenizer = transformers.AutoTokenizer.from_pretrained(
    model_name, model_max_tokens=2048, use_fast=False,
    padding_side="right"
)
tokenizer.pad_token = tokenizer.unk_token

def prompt_template(prompt):
    return f"""<s>[INST]<<sys>>You are a helpful assistant<</sys>>
        {prompt}
        [/INST]"""

# Test case
prompt = prompt_template("who is referred to as Godfather of AI?")
print(Fore.CYAN + prompt)
tokens = tokenizer.encode(prompt, return_tensors='pt').to(device)
response = model.generate(tokens)
print(Fore.MAGENTA + tokenizer.decode(response[0]))

# Get the reft model
reft_config = pyreft.ReftConfig(
    representations={
        "layer":15,
        "component":"block_output",
        "low_rank_dimension":4,
        "intervention":pyreft.LoreftIntervention(
            embed_dim=model.config.hidden_size, low_rank_dimension=4
        )
    }
)

reft_model = pyreft.get_reft_model(model, reft_config)
reft_model.set_device(device)


data_module = pyreft.make_last_position_supervised_data_module(
    tokenizer,
    model,
    [prompt_template(x) for x in X],
    y
)

# Training arguments
training_arguments = transformers.TrainingArguments(
    num_train_epochs=150,
    output_dir = output_dir,
    per_device_train_batch_size=2,
    learning_rate=2e-3,
    logging_steps=20,
    report_to=[]
)

# Trainer for the reft model
trainer = pyreft.ReftTrainerForCausalLM(
    model=reft_model,
    tokenizer=tokenizer,
    args=training_arguments,
    **data_module
)

# Train the model!!
_ = trainer.train()

reft_model.set_device(device)

import torch, transformers, pyreft
from colorama import init, Fore
device = 'cuda' if torch.cuda.is_available() else 'cpu'
init()
def Input_prompt(prompt):
    return f"""<s>[INST]<<sys>>You are a helpful assistant<</sys>>
        {prompt}
        [/INST]"""
# user_input = input("Please enter your prompt: ")

# Test case
prompt = Input_prompt("What are recent views of Geoffrey Hinton on future of AI & humanity?")
tokens = tokenizer(prompt, return_tensors='pt').to(device)

# Generate a prediction
base_unit_position = tokens['input_ids'].shape[-1] -1
_, response = reft_model.generate(tokens,
                            unit_locations={'sources->base':(None, [[[base_unit_position]]])},
                            intervene_on_prompt=True
                            )
print(Fore.LIGHTGREEN_EX + tokenizer.decode(response[0]))

def Ask_anything():
  user_input = input("Please enter your prompt: ")
  prompt = Input_prompt(user_input)
  tokens = tokenizer(prompt, return_tensors='pt').to(device)
  base_unit_position = tokens['input_ids'].shape[-1] -1
  _, response = reft_model.generate(tokens,
                              unit_locations={'sources->base':(None, [[[base_unit_position]]])},
                              intervene_on_prompt=True
                              )
  print(Fore.LIGHTGREEN_EX + tokenizer.decode(response[0]))

Ask_anything()